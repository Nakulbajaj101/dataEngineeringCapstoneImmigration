{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of this project to create an open analytical database for consumption by US Government so they are able to understand immigration patterns,\n",
    "  in order for them to make and monitor policy decisions around immigration so that industries dependent on immigration such as tourism, business and international student industry are able to strive successfully. The immigration patterns can further help in making infrastructure based decisions. The analytical database should serve daily to long-term decision making needs.\n",
    "  \n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from datetime import datetime, timedelta\n",
    "from i94MetadataMappings import visa_codes, ports_codes, cit_and_res_codes, mode_codes\n",
    "from helperFunctions import explore_dataframe, clean_latitude_longitude, clean_date, split_extract\n",
    "from pyspark.sql import SparkSession, Window, SQLContext\n",
    "from pyspark.sql.functions import (datediff,isnan, udf,mean, upper, first, last, col, sum as Sum, ltrim, rtrim,concat,\n",
    "                                  row_number, when, date_format, dayofmonth, dayofweek,\n",
    "                                  hour, lit, month, row_number, to_date, weekofyear,\n",
    "                                  year, split, max as Max, min as Min)\n",
    "from pyspark.sql.types import TimestampType, IntegerType, StringType, FloatType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import Image\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project an analytical database will be made available for the USA government, so they can quickly gather insights from the data which may enable them to:\n",
    "\n",
    "1. Make policy decisions on immigration.\n",
    "2. Monitor their policy decisions, and the impact of policy decisions in the USA.\n",
    "3. Optimise infrastructure spending based on immigration patterns.\n",
    "\n",
    "\n",
    "##### Data\n",
    "The following data was leveraged to build the database:\n",
    "\n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace.\n",
    "* World Temperature Data: This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "* U.S. City Demographic Data: This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "* Airport Code Table: This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "##### Solution\n",
    "\n",
    "A spark etl job was created to read the data and write to s3\n",
    "bucket using emr cluster on AWS. Then the second job which is outside the scope of the \n",
    "project can be run to pick data from aws s3 bucket and load it into redshift cluster.\n",
    "The cluster end point will be made available for analytics team of the government.\n",
    "\n",
    "The analytical database will levereage the star schema design for query optimisation with immigration data held in fact table and others will be dimension tables. The following are the tables created with brief summary:\n",
    "\n",
    "\n",
    "* regions_dm: (State/region wise aggregated demographic information)\n",
    "* airports_dm: (\n",
    "* airlines_dm: (airlines and flights information)\n",
    "* regions_dm: (countries/regions with monthly temperature data and latitude longitude)\n",
    "* ports_dm: (port codes and names)\n",
    "* modes_dm: (mode of transport names)\n",
    "* visas_dm: (visa codes and purpose)\n",
    "* immi_dates_dm: (arrival and departure dates, with other date related information such as days, day of week, year, month, etc)\n",
    "* immigration_ft: Immigration fact table for daily immigration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "airport_data = pd.read_csv(\"airport-codes_csv.csv\")\n",
    "immigration_data = pd.read_csv(\"immigration_data_sample.csv\")\n",
    "demographics_data = pd.read_csv(\"us-cities-demographics.csv\", sep=\";\")\n",
    "climate_file_path = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temperature_data = pd.read_csv(climate_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Running the pandas explore script function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Exploring the airports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 55075 rows and 12 columns\n",
      "\n",
      "The data types are : ident            object\n",
      "type             object\n",
      "name             object\n",
      "elevation_ft    float64\n",
      "continent        object\n",
      "iso_country      object\n",
      "iso_region       object\n",
      "municipality     object\n",
      "gps_code         object\n",
      "iata_code        object\n",
      "local_code       object\n",
      "coordinates      object\n",
      "dtype: object\n",
      "\n",
      "Showing number of missing records per column\n",
      "ident               0\n",
      "type                0\n",
      "name                0\n",
      "elevation_ft     7006\n",
      "continent       27719\n",
      "iso_country       247\n",
      "iso_region          0\n",
      "municipality     5676\n",
      "gps_code        14045\n",
      "iata_code       45886\n",
      "local_code      26389\n",
      "coordinates         0\n",
      "dtype: int64\n",
      "\n",
      "Length of dataframe is 55075\n",
      "The number of unique rows for column ident are 55075\n",
      "The number of unique rows for column type are 7\n",
      "The number of unique rows for column name are 52144\n",
      "The number of unique rows for column elevation_ft are 5450\n",
      "The number of unique rows for column continent are 7\n",
      "The number of unique rows for column iso_country are 244\n",
      "The number of unique rows for column iso_region are 2810\n",
      "The number of unique rows for column municipality are 27134\n",
      "The number of unique rows for column gps_code are 40851\n",
      "The number of unique rows for column iata_code are 9043\n",
      "The number of unique rows for column local_code are 27437\n",
      "The number of unique rows for column coordinates are 54874\n",
      "\n",
      "Top 10 rows of the data are: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00AS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Fulton Airport</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-OK</td>\n",
       "      <td>Alex</td>\n",
       "      <td>00AS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AS</td>\n",
       "      <td>-97.8180194, 34.9428028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00AZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Cordes Airport</td>\n",
       "      <td>3810.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>Cordes</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>-112.16500091552734, 34.305599212646484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00CA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Goldstone /Gts/ Airport</td>\n",
       "      <td>3038.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Barstow</td>\n",
       "      <td>00CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CA</td>\n",
       "      <td>-116.888000488, 35.350498199499995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00CL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Williams Ag Airport</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Biggs</td>\n",
       "      <td>00CL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CL</td>\n",
       "      <td>-121.763427, 39.427188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00CN</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Kitchen Creek Helibase Heliport</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Pine Valley</td>\n",
       "      <td>00CN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CN</td>\n",
       "      <td>-116.4597417, 32.7273736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "5  00AS  small_airport                      Fulton Airport        1100.0   \n",
       "6  00AZ  small_airport                      Cordes Airport        3810.0   \n",
       "7  00CA  small_airport             Goldstone /Gts/ Airport        3038.0   \n",
       "8  00CL  small_airport                 Williams Ag Airport          87.0   \n",
       "9  00CN       heliport     Kitchen Creek Helibase Heliport        3350.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "5       NaN          US      US-OK          Alex     00AS       NaN   \n",
       "6       NaN          US      US-AZ        Cordes     00AZ       NaN   \n",
       "7       NaN          US      US-CA       Barstow     00CA       NaN   \n",
       "8       NaN          US      US-CA         Biggs     00CL       NaN   \n",
       "9       NaN          US      US-CA   Pine Valley     00CN       NaN   \n",
       "\n",
       "  local_code                              coordinates  \n",
       "0        00A       -74.93360137939453, 40.07080078125  \n",
       "1       00AA                   -101.473911, 38.704022  \n",
       "2       00AK              -151.695999146, 59.94919968  \n",
       "3       00AL    -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                      -91.254898, 35.6087  \n",
       "5       00AS                  -97.8180194, 34.9428028  \n",
       "6       00AZ  -112.16500091552734, 34.305599212646484  \n",
       "7       00CA       -116.888000488, 35.350498199499995  \n",
       "8       00CL                   -121.763427, 39.427188  \n",
       "9       00CN                 -116.4597417, 32.7273736  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_dataframe(airport_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Exploring the Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 1000 rows and 29 columns\n",
      "\n",
      "The data types are : Unnamed: 0      int64\n",
      "cicid         float64\n",
      "i94yr         float64\n",
      "i94mon        float64\n",
      "i94cit        float64\n",
      "i94res        float64\n",
      "i94port        object\n",
      "arrdate       float64\n",
      "i94mode       float64\n",
      "i94addr        object\n",
      "depdate       float64\n",
      "i94bir        float64\n",
      "i94visa       float64\n",
      "count         float64\n",
      "dtadfile        int64\n",
      "visapost       object\n",
      "occup          object\n",
      "entdepa        object\n",
      "entdepd        object\n",
      "entdepu       float64\n",
      "matflag        object\n",
      "biryear       float64\n",
      "dtaddto        object\n",
      "gender         object\n",
      "insnum        float64\n",
      "airline        object\n",
      "admnum        float64\n",
      "fltno          object\n",
      "visatype       object\n",
      "dtype: object\n",
      "\n",
      "Showing number of missing records per column\n",
      "Unnamed: 0       0\n",
      "cicid            0\n",
      "i94yr            0\n",
      "i94mon           0\n",
      "i94cit           0\n",
      "i94res           0\n",
      "i94port          0\n",
      "arrdate          0\n",
      "i94mode          0\n",
      "i94addr         59\n",
      "depdate         49\n",
      "i94bir           0\n",
      "i94visa          0\n",
      "count            0\n",
      "dtadfile         0\n",
      "visapost       618\n",
      "occup          996\n",
      "entdepa          0\n",
      "entdepd         46\n",
      "entdepu       1000\n",
      "matflag         46\n",
      "biryear          0\n",
      "dtaddto          0\n",
      "gender         141\n",
      "insnum         965\n",
      "airline         33\n",
      "admnum           0\n",
      "fltno            8\n",
      "visatype         0\n",
      "dtype: int64\n",
      "\n",
      "Length of dataframe is 1000\n",
      "The number of unique rows for column Unnamed: 0 are 1000\n",
      "The number of unique rows for column cicid are 1000\n",
      "The number of unique rows for column i94yr are 1\n",
      "The number of unique rows for column i94mon are 1\n",
      "The number of unique rows for column i94cit are 88\n",
      "The number of unique rows for column i94res are 91\n",
      "The number of unique rows for column i94port are 70\n",
      "The number of unique rows for column arrdate are 30\n",
      "The number of unique rows for column i94mode are 4\n",
      "The number of unique rows for column i94addr are 52\n",
      "The number of unique rows for column depdate are 110\n",
      "The number of unique rows for column i94bir are 85\n",
      "The number of unique rows for column i94visa are 3\n",
      "The number of unique rows for column count are 1\n",
      "The number of unique rows for column dtadfile are 39\n",
      "The number of unique rows for column visapost are 98\n",
      "The number of unique rows for column occup are 4\n",
      "The number of unique rows for column entdepa are 9\n",
      "The number of unique rows for column entdepd are 11\n",
      "The number of unique rows for column entdepu are 1\n",
      "The number of unique rows for column matflag are 2\n",
      "The number of unique rows for column biryear are 85\n",
      "The number of unique rows for column dtaddto are 99\n",
      "The number of unique rows for column gender are 4\n",
      "The number of unique rows for column insnum are 30\n",
      "The number of unique rows for column airline are 102\n",
      "The number of unique rows for column admnum are 1000\n",
      "The number of unique rows for column fltno are 503\n",
      "The number of unique rows for column visatype are 10\n",
      "\n",
      "Top 10 rows of the data are: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160423</td>\n",
       "      <td>MTR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>DOH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>721257</td>\n",
       "      <td>1481650.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20552.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>20606.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>10072016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DL</td>\n",
       "      <td>7.368526e+08</td>\n",
       "      <td>910</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1072780</td>\n",
       "      <td>2197173.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20635.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>10112016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CX</td>\n",
       "      <td>7.863122e+08</td>\n",
       "      <td>870</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>112205</td>\n",
       "      <td>232708.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20546.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20554.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>06302016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BA</td>\n",
       "      <td>5.547449e+10</td>\n",
       "      <td>00117</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2577162</td>\n",
       "      <td>5227851.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>20575.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>07262016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LX</td>\n",
       "      <td>5.941342e+10</td>\n",
       "      <td>00008</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10930</td>\n",
       "      <td>13213.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>06292016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>5.544979e+10</td>\n",
       "      <td>00109</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "5      721257  1481650.0  2016.0     4.0   577.0   577.0     ATL  20552.0   \n",
       "6     1072780  2197173.0  2016.0     4.0   245.0   245.0     SFR  20556.0   \n",
       "7      112205   232708.0  2016.0     4.0   113.0   135.0     NYC  20546.0   \n",
       "8     2577162  5227851.0  2016.0     4.0   131.0   131.0     CHI  20572.0   \n",
       "9       10930    13213.0  2016.0     4.0   116.0   116.0     LOS  20545.0   \n",
       "\n",
       "   i94mode i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup  \\\n",
       "0      1.0      HI  20573.0    61.0      2.0    1.0  20160422      NaN   NaN   \n",
       "1      1.0      TX  20568.0    26.0      2.0    1.0  20160423      MTR   NaN   \n",
       "2      1.0      FL  20571.0    76.0      2.0    1.0  20160407      NaN   NaN   \n",
       "3      1.0      CA  20581.0    25.0      2.0    1.0  20160428      DOH   NaN   \n",
       "4      3.0      NY  20553.0    19.0      2.0    1.0  20160406      NaN   NaN   \n",
       "5      1.0      GA  20606.0    51.0      2.0    1.0  20160408      NaN   NaN   \n",
       "6      1.0      CA  20635.0    48.0      2.0    1.0  20160412      NaN   NaN   \n",
       "7      1.0      NY  20554.0    33.0      2.0    1.0  20160402      NaN   NaN   \n",
       "8      1.0      IL  20575.0    39.0      2.0    1.0  20160428      NaN   NaN   \n",
       "9      1.0      CA  20553.0    35.0      2.0    1.0  20160401      NaN   NaN   \n",
       "\n",
       "  entdepa entdepd  entdepu matflag  biryear   dtaddto gender  insnum airline  \\\n",
       "0       G       O      NaN       M   1955.0  07202016      F     NaN      JL   \n",
       "1       G       R      NaN       M   1990.0  10222016      M     NaN     *GA   \n",
       "2       G       O      NaN       M   1940.0  07052016      M     NaN      LH   \n",
       "3       G       O      NaN       M   1991.0  10272016      M     NaN      QR   \n",
       "4       Z       K      NaN       M   1997.0  07042016      F     NaN     NaN   \n",
       "5       T       N      NaN       M   1965.0  10072016      M     NaN      DL   \n",
       "6       T       O      NaN       M   1968.0  10112016      F     NaN      CX   \n",
       "7       G       O      NaN       M   1983.0  06302016      F     NaN      BA   \n",
       "8       O       O      NaN       M   1977.0  07262016    NaN     NaN      LX   \n",
       "9       O       O      NaN       M   1981.0  06292016    NaN     NaN      AA   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  5.658267e+10  00782       WT  \n",
       "1  9.436200e+10  XBLNG       B2  \n",
       "2  5.578047e+10  00464       WT  \n",
       "3  9.478970e+10  00739       B2  \n",
       "4  4.232257e+10   LAND       WT  \n",
       "5  7.368526e+08    910       B2  \n",
       "6  7.863122e+08    870       B2  \n",
       "7  5.547449e+10  00117       WT  \n",
       "8  5.941342e+10  00008       WT  \n",
       "9  5.544979e+10  00109       WT  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_dataframe(immigration_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Exploring the Demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 2891 rows and 12 columns\n",
      "\n",
      "The data types are : City                       object\n",
      "State                      object\n",
      "Median Age                float64\n",
      "Male Population           float64\n",
      "Female Population         float64\n",
      "Total Population            int64\n",
      "Number of Veterans        float64\n",
      "Foreign-born              float64\n",
      "Average Household Size    float64\n",
      "State Code                 object\n",
      "Race                       object\n",
      "Count                       int64\n",
      "dtype: object\n",
      "\n",
      "Showing number of missing records per column\n",
      "City                       0\n",
      "State                      0\n",
      "Median Age                 0\n",
      "Male Population            3\n",
      "Female Population          3\n",
      "Total Population           0\n",
      "Number of Veterans        13\n",
      "Foreign-born              13\n",
      "Average Household Size    16\n",
      "State Code                 0\n",
      "Race                       0\n",
      "Count                      0\n",
      "dtype: int64\n",
      "\n",
      "Length of dataframe is 2891\n",
      "The number of unique rows for column City are 567\n",
      "The number of unique rows for column State are 49\n",
      "The number of unique rows for column Median Age are 180\n",
      "The number of unique rows for column Male Population are 594\n",
      "The number of unique rows for column Female Population are 595\n",
      "The number of unique rows for column Total Population are 594\n",
      "The number of unique rows for column Number of Veterans are 578\n",
      "The number of unique rows for column Foreign-born are 588\n",
      "The number of unique rows for column Average Household Size are 162\n",
      "The number of unique rows for column State Code are 49\n",
      "The number of unique rows for column Race are 5\n",
      "The number of unique rows for column Count are 2785\n",
      "\n",
      "Top 10 rows of the data are: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>33.1</td>\n",
       "      <td>56229.0</td>\n",
       "      <td>62432.0</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634.0</td>\n",
       "      <td>7517.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avondale</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>29.1</td>\n",
       "      <td>38712.0</td>\n",
       "      <td>41971.0</td>\n",
       "      <td>80683</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>8355.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>11592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Covina</td>\n",
       "      <td>California</td>\n",
       "      <td>39.8</td>\n",
       "      <td>51629.0</td>\n",
       "      <td>56860.0</td>\n",
       "      <td>108489</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>37038.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>32716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41762.0</td>\n",
       "      <td>43270.0</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783.0</td>\n",
       "      <td>3269.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High Point</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>35.5</td>\n",
       "      <td>51751.0</td>\n",
       "      <td>58077.0</td>\n",
       "      <td>109828</td>\n",
       "      <td>5204.0</td>\n",
       "      <td>16315.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>11060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City           State  Median Age  Male Population  \\\n",
       "0     Silver Spring        Maryland        33.8          40601.0   \n",
       "1            Quincy   Massachusetts        41.0          44129.0   \n",
       "2            Hoover         Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga      California        34.5          88127.0   \n",
       "4            Newark      New Jersey        34.6         138040.0   \n",
       "5            Peoria        Illinois        33.1          56229.0   \n",
       "6          Avondale         Arizona        29.1          38712.0   \n",
       "7       West Covina      California        39.8          51629.0   \n",
       "8          O'Fallon        Missouri        36.0          41762.0   \n",
       "9        High Point  North Carolina        35.5          51751.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "5            62432.0            118661              6634.0        7517.0   \n",
       "6            41971.0             80683              4815.0        8355.0   \n",
       "7            56860.0            108489              3800.0       37038.0   \n",
       "8            43270.0             85032              5783.0        3269.0   \n",
       "9            58077.0            109828              5204.0       16315.0   \n",
       "\n",
       "   Average Household Size State Code                               Race  Count  \n",
       "0                    2.60         MD                 Hispanic or Latino  25924  \n",
       "1                    2.39         MA                              White  58723  \n",
       "2                    2.58         AL                              Asian   4759  \n",
       "3                    3.18         CA          Black or African-American  24437  \n",
       "4                    2.73         NJ                              White  76402  \n",
       "5                    2.40         IL  American Indian and Alaska Native   1343  \n",
       "6                    3.18         AZ          Black or African-American  11592  \n",
       "7                    3.56         CA                              Asian  32716  \n",
       "8                    2.77         MO                 Hispanic or Latino   2583  \n",
       "9                    2.65         NC                              Asian  11060  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_dataframe(demographics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Exploring the climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 8599212 rows and 7 columns\n",
      "\n",
      "The data types are : dt                                object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                              object\n",
      "Country                           object\n",
      "Latitude                          object\n",
      "Longitude                         object\n",
      "dtype: object\n",
      "\n",
      "Showing number of missing records per column\n",
      "dt                                    0\n",
      "AverageTemperature               364130\n",
      "AverageTemperatureUncertainty    364130\n",
      "City                                  0\n",
      "Country                               0\n",
      "Latitude                              0\n",
      "Longitude                             0\n",
      "dtype: int64\n",
      "\n",
      "Length of dataframe is 8599212\n",
      "The number of unique rows for column dt are 3239\n",
      "The number of unique rows for column AverageTemperature are 111995\n",
      "The number of unique rows for column AverageTemperatureUncertainty are 10903\n",
      "The number of unique rows for column City are 3448\n",
      "The number of unique rows for column Country are 159\n",
      "The number of unique rows for column Latitude are 73\n",
      "The number of unique rows for column Longitude are 1227\n",
      "\n",
      "Top 10 rows of the data are: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1744-04-01</td>\n",
       "      <td>5.788</td>\n",
       "      <td>3.624</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1744-05-01</td>\n",
       "      <td>10.644</td>\n",
       "      <td>1.283</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1744-06-01</td>\n",
       "      <td>14.051</td>\n",
       "      <td>1.347</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1744-07-01</td>\n",
       "      <td>16.082</td>\n",
       "      <td>1.396</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1744-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  rhus   \n",
       "1  1743-12-01                 NaN                            NaN  rhus   \n",
       "2  1744-01-01                 NaN                            NaN  rhus   \n",
       "3  1744-02-01                 NaN                            NaN  rhus   \n",
       "4  1744-03-01                 NaN                            NaN  rhus   \n",
       "5  1744-04-01               5.788                          3.624  rhus   \n",
       "6  1744-05-01              10.644                          1.283  rhus   \n",
       "7  1744-06-01              14.051                          1.347  rhus   \n",
       "8  1744-07-01              16.082                          1.396  rhus   \n",
       "9  1744-08-01                 NaN                            NaN  rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  \n",
       "5  Denmark   57.05N    10.33E  \n",
       "6  Denmark   57.05N    10.33E  \n",
       "7  Denmark   57.05N    10.33E  \n",
       "8  Denmark   57.05N    10.33E  \n",
       "9  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_dataframe(temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating or getting the spark session\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"36000\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .appName(\"SparkImmigration\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Defining the sql context \n",
    "sc = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists, hence reading data from directory sas_data\n"
     ]
    }
   ],
   "source": [
    "#Checking if the data exists , else getting data from another location\n",
    "folder_name = \"sas_data\"\n",
    "if not os.path.exists(os.getcwd() + f\"/{folder_name}\"):\n",
    "    print(\"Reading data from source\")\n",
    "    df_immigration_data = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "    df_immigration_data.write.parquet(f\"{folder_name}\")\n",
    "else:\n",
    "    print(f\"Folder already exists, hence reading data from directory {folder_name}\")\n",
    "    df_immigration_data = spark.read.parquet(f\"{folder_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Getting the mappings visa, ports and mode of transaport codes and loading in their own database\n",
    "df_visa_codes = spark.createDataFrame(list(map(list, visa_codes.items())),\n",
    "                                        [\"code\",\"travel_purpose\"])\n",
    "                                        \n",
    "df_ports_codes = spark.createDataFrame(list(map(list, ports_codes.items())),\n",
    "                                         [\"code\", \"port_name\"])\n",
    "\n",
    "#Lets clean the ports and remove any left and right trailing spaces\n",
    "df_ports_codes = df_ports_codes.withColumn(\"port_name\", ltrim(col(\"port_name\")))\\\n",
    "                               .withColumn(\"port_name\", rtrim(col(\"port_name\")))\n",
    "                                         \n",
    "df_mode_codes = spark.createDataFrame(list(map(list, mode_codes.items())),\n",
    "                                        [\"code\",\"travel_mode\"])                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#user defined functions and window function definition\n",
    "\n",
    "get_datetime = udf(lambda date : (timedelta(days=date) + datetime(1960,1,1)) if date > 0.0 else None, TimestampType())\n",
    "\n",
    "cast_integer = udf(lambda val: int(val) if val != 0 else np.NaN , IntegerType())\n",
    "\n",
    "to_split_extract_string = udf(split_extract, StringType())\n",
    "\n",
    "to_split_extract_float = udf(split_extract,FloatType())\n",
    "\n",
    "cast_lat_lon = udf(clean_latitude_longitude, FloatType())\n",
    "\n",
    "window = Window.orderBy(col(\"airline_code\"), col(\"flight_number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the schema of the immigration data\n",
    "df_immigration_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The dates are in double, hence need to be converted to date for better analysis. A lot of columns are not used by the government and hence will be dropped. These are listed below:\n",
    "\n",
    " DTADFILE \n",
    "\n",
    "\n",
    " VISAPOST \n",
    "\n",
    "\n",
    " OCCUP \n",
    "\n",
    "\n",
    " ENTDEPA \n",
    "\n",
    "\n",
    " ENTDEPD\n",
    "\n",
    "\n",
    " ENTDEPU \n",
    "\n",
    "\n",
    " DTADDTO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at number of cicid unique records\n",
    "df_immigration_data.select(\"cicid\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if unique records are same as total records\n",
    "df_immigration_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Since cicid unique records are same as number of records, we can assume there is no duplicate data.\n",
    "\n",
    "Lets clean and process immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cleaning the arrival and departure dates, and if dates are null assigning 0 date\n",
    "df_immigration_data = df_immigration_data.withColumn(\"arrdate\", get_datetime(df_immigration_data.arrdate))\\\n",
    "                               .withColumn(\"depdate\", when((df_immigration_data.depdate.isNull()) , 0.0)\\\n",
    "                               .otherwise(df_immigration_data.depdate))\n",
    "\n",
    "df_immigration_data = df_immigration_data.withColumn(\"depdate\",get_datetime(df_immigration_data.depdate))\n",
    "\n",
    "#converting double columns to integer\n",
    "float_immi_columns = {_[0]:_[1] for _ in df_immigration_data.dtypes if _[1] == 'double'}\n",
    "columns = [_ for _ in float_immi_columns.keys()]\n",
    "df_immigration_data = df_immigration_data.fillna(0, subset=columns)\n",
    "for _ in columns:\n",
    "    df_immigration_data = df_immigration_data.withColumn(_, cast_integer(df_immigration_data[_]))\n",
    "\n",
    "#extracting unique flights data and creating a flights table with the unique identifier\n",
    "df_flights_data = df_immigration_data.selectExpr(\"airline as airline_code\",\"fltno as flight_number\")\\\n",
    "                           .dropDuplicates()\\\n",
    "                           .na\\\n",
    "                           .drop(subset=[\"airline_code\",\"flight_number\"])\n",
    "\n",
    "df_flights_data = df_flights_data.withColumn(\"flight_id\", row_number().over(window) )\n",
    "\n",
    "\n",
    "#joining the immigration data back to flights data\n",
    "df_immigration_data = df_flights_data.selectExpr(\"airline_code as airline\",\"flight_number as fltno\",\"flight_id\").join(df_immigration_data, on=[\"airline\", \"fltno\"], how=\"left\")\n",
    "\n",
    "df_immigration_data = df_immigration_data.selectExpr(\"cicid\",\\\n",
    "                         \"i94cit as city_code\",\\\n",
    "                         \"i94res as res_code\",\\\n",
    "                         \"i94port as port\",\\\n",
    "                         \"arrdate as arr_date\",\\\n",
    "                         \"i94mode as mode\",\\\n",
    "                         \"i94addr as addr\",\\\n",
    "                         \"depdate as dept_date\",\\\n",
    "                         \"i94bir as age\",\\\n",
    "                         \"i94visa as visa_code\",\\\n",
    "                         \"count as counter\",\\\n",
    "                         \"matflag as matflag\",\\\n",
    "                         \"biryear as birth_year\",\\\n",
    "                         \"gender\",\\\n",
    "                         \"flight_id\",\\\n",
    "                         \"i94yr as date_year\",\\\n",
    "                         \"i94mon as date_month\",\\\n",
    "                         \"visatype as visa_type\",\\\n",
    "                         \"admnum as admission_number\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dept_year</th>\n",
       "      <th>arr_year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>124587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>2887203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dept_year  arr_year    count\n",
       "0     2012.0      2016        2\n",
       "1     2015.0      2016        3\n",
       "2        NaN      2016   124587\n",
       "3     2016.0      2016  2887203"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets explore how many values by year\n",
    "df_immigration_data.select(year(\"dept_date\").alias(\"dept_year\"),year(\"arr_date\").alias(\"arr_year\"))\\\n",
    "                   .groupBy(\"dept_year\",\"arr_year\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the data extracted should be limiting 2016 april, but dept_year has some dates going back to 2012 and 2015,\n",
    "these should be filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_data = df_immigration_data.filter(\"year(dept_date) = 2016 or dept_date IS NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We should also ensure that dept_date cannot be before arrival date, and keep null dept_date ,since these people are likely to be in the country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_data = df_immigration_data.withColumn(\"diff\", datediff(\"dept_date\",\"arr_date\"))\\\n",
    "                                        .filter(\"diff >= 0 or dept_date is NULL\")\\\n",
    "                                        .drop(\"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>124587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>2886939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year    count\n",
       "0     NaN   124587\n",
       "1  2016.0  2886939"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirming we have 2016 dates and NaN date\n",
    "df_immigration_data.select(year(\"dept_date\").alias(\"year\")).groupBy(\"year\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Extracting unique dates from arrival and departure to be used for further analysis\n",
    "df_immigration_data.select(\"arr_date\",\"dept_date\")\\\n",
    "              .filter(\"dept_date IS NOT NULL\")\\\n",
    "              .dropDuplicates(subset=[\"arr_date\",\"dept_date\"])\\\n",
    "              .registerTempTable(\"temp_date_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>city_code</th>\n",
       "      <th>res_code</th>\n",
       "      <th>port</th>\n",
       "      <th>arr_date</th>\n",
       "      <th>mode</th>\n",
       "      <th>addr</th>\n",
       "      <th>dept_date</th>\n",
       "      <th>age</th>\n",
       "      <th>visa_code</th>\n",
       "      <th>counter</th>\n",
       "      <th>matflag</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>flight_id</th>\n",
       "      <th>date_year</th>\n",
       "      <th>date_month</th>\n",
       "      <th>visa_type</th>\n",
       "      <th>admission_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74495</td>\n",
       "      <td>254</td>\n",
       "      <td>276</td>\n",
       "      <td>ANC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AK</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1972</td>\n",
       "      <td>M</td>\n",
       "      <td>200</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>WB</td>\n",
       "      <td>-429925015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77160</td>\n",
       "      <td>254</td>\n",
       "      <td>276</td>\n",
       "      <td>ANC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AK</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1978</td>\n",
       "      <td>M</td>\n",
       "      <td>200</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>B1</td>\n",
       "      <td>-2069752582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4013911</td>\n",
       "      <td>687</td>\n",
       "      <td>687</td>\n",
       "      <td>FTL</td>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>1</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1992</td>\n",
       "      <td>M</td>\n",
       "      <td>222</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>B1</td>\n",
       "      <td>-339484682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4013912</td>\n",
       "      <td>687</td>\n",
       "      <td>687</td>\n",
       "      <td>FTL</td>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>1</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1965</td>\n",
       "      <td>M</td>\n",
       "      <td>222</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>B1</td>\n",
       "      <td>-339606582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6072603</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>LAR</td>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>1</td>\n",
       "      <td>TX</td>\n",
       "      <td>2016-06-24</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1956</td>\n",
       "      <td>M</td>\n",
       "      <td>303</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>B2</td>\n",
       "      <td>-570672994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cicid  city_code  res_code port   arr_date  mode addr  dept_date  age  \\\n",
       "0    74495        254       276  ANC 2016-04-01     1   AK 2016-04-03   44   \n",
       "1    77160        254       276  ANC 2016-04-01     1   AK 2016-04-03   38   \n",
       "2  4013911        687       687  FTL 2016-04-21     1   FL 2016-04-23   24   \n",
       "3  4013912        687       687  FTL 2016-04-21     1   FL 2016-04-23   51   \n",
       "4  6072603        582       582  LAR 2016-04-18     1   TX 2016-06-24   60   \n",
       "\n",
       "   visa_code  counter matflag  birth_year gender  flight_id  date_year  \\\n",
       "0          1        1       M        1972      M        200       2016   \n",
       "1          1        1       M        1978      M        200       2016   \n",
       "2          1        1       M        1992      M        222       2016   \n",
       "3          1        1       M        1965      M        222       2016   \n",
       "4          2        1       M        1956      M        303       2016   \n",
       "\n",
       "   date_month visa_type  admission_number  \n",
       "0           4        WB        -429925015  \n",
       "1           4        B1       -2069752582  \n",
       "2           4        B1        -339484682  \n",
       "3           4        B1        -339606582  \n",
       "4           4        B2        -570672994  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the top 5 rows of final immigartion\n",
    "df_immigration_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_code</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>flight_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*FF</td>\n",
       "      <td>00001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*FF</td>\n",
       "      <td>00052</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*FF</td>\n",
       "      <td>11626</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*FF</td>\n",
       "      <td>354</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*FF</td>\n",
       "      <td>4520</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_code flight_number  flight_id\n",
       "0          *FF         00001          1\n",
       "1          *FF         00052          2\n",
       "2          *FF         11626          3\n",
       "3          *FF           354          4\n",
       "4          *FF          4520          5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the 5 rows of flight data extracted\n",
    "df_flights_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### The airport data explored with pandas above has no duplicates as identity record matches the length of the dataframe, however there are few columns with lot of missing values. Since the column that will be used to map to the immigration data, which is IATA_CODE, and has 45886 missing rows out of 55075 rows. Hence we will remove all the rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read in airports data into spark dataframe\n",
    "df_airports_data = spark.read.format('csv').option(\"header\",True).load(\"airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Dropping the null values for iata_code\n",
    "df_airports_data = df_airports_data.dropna(subset=[\"iata_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>9189</td>\n",
       "      <td>9189</td>\n",
       "      <td>9189</td>\n",
       "      <td>8819</td>\n",
       "      <td>9189</td>\n",
       "      <td>9189</td>\n",
       "      <td>9189</td>\n",
       "      <td>8423</td>\n",
       "      <td>8538</td>\n",
       "      <td>9189</td>\n",
       "      <td>2987</td>\n",
       "      <td>9189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1146.6716180972899</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22077.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1715.6977018420007</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56.568542494923804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41040.010051168356</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>03N</td>\n",
       "      <td>closed</td>\n",
       "      <td>\"Aeropuerto \"\"General Tomas de Heres\"\". Ciudad...</td>\n",
       "      <td>-11</td>\n",
       "      <td>AF</td>\n",
       "      <td>AD</td>\n",
       "      <td>AD-07</td>\n",
       "      <td>108 Mile</td>\n",
       "      <td>00F</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-0.005456000100821257, 16.24839973449707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>rjns</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>ilina Airport</td>\n",
       "      <td>999</td>\n",
       "      <td>SA</td>\n",
       "      <td>ZW</td>\n",
       "      <td>ZW-MW</td>\n",
       "      <td>ilina</td>\n",
       "      <td>ZYYK</td>\n",
       "      <td>ZZV</td>\n",
       "      <td>ZZV</td>\n",
       "      <td>99.951499939, 12.6361999512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary ident           type  \\\n",
       "0   count  9189           9189   \n",
       "1    mean  None           None   \n",
       "2  stddev  None           None   \n",
       "3     min   03N         closed   \n",
       "4     max  rjns  small_airport   \n",
       "\n",
       "                                                name        elevation_ft  \\\n",
       "0                                               9189                8819   \n",
       "1                                               None  1146.6716180972899   \n",
       "2                                               None  1715.6977018420007   \n",
       "3  \"Aeropuerto \"\"General Tomas de Heres\"\". Ciudad...                 -11   \n",
       "4                                    ilina Airport                 999   \n",
       "\n",
       "  continent iso_country iso_region municipality            gps_code iata_code  \\\n",
       "0      9189        9189       9189         8423                8538      9189   \n",
       "1      None        None       None         None                40.0       0.0   \n",
       "2      None        None       None         None  56.568542494923804       0.0   \n",
       "3        AF          AD      AD-07     108 Mile                 00F         -   \n",
       "4        SA          ZW      ZW-MW      ilina                ZYYK       ZZV   \n",
       "\n",
       "           local_code                               coordinates  \n",
       "0                2987                                      9189  \n",
       "1             22077.5                                      None  \n",
       "2  41040.010051168356                                      None  \n",
       "3                   -  -0.005456000100821257, 16.24839973449707  \n",
       "4                 ZZV               99.951499939, 12.6361999512  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets summarise the airports data to see if after removing iata_code with nulls, data looks better, and less missing records across other fields\n",
    "df_airports_data.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### The missing values have decreased significantly for other columns. ISO_COUNTRY, CONTINENT, ISO_REGION and COORDINATES columns, which might be essential for analytics, have full data. Lets now confirm that IATA_CODE has no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9042"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the unique values for IATA_CODE\n",
    "df_airports_data.select(\"iata_code\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata_code</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BR</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OHE</td>\n",
       "      <td>CN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRI</td>\n",
       "      <td>SC</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NWT</td>\n",
       "      <td>PG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BVW</td>\n",
       "      <td>AU</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CMN</td>\n",
       "      <td>MA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFY</td>\n",
       "      <td>MA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GGC</td>\n",
       "      <td>AO</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PCO</td>\n",
       "      <td>MX</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IZA</td>\n",
       "      <td>BR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iata_code iso_country  count\n",
       "0         0          BR     80\n",
       "1       OHE          CN      3\n",
       "2       PRI          SC      3\n",
       "3       NWT          PG      2\n",
       "4       BVW          AU      2\n",
       "5       CMN          MA      2\n",
       "6       TFY          MA      2\n",
       "7       GGC          AO      2\n",
       "8       PCO          MX      2\n",
       "9       IZA          BR      2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since the unique values are less than total values, there are some duplicated codes. \n",
    "#Lets explore and check which codes are mostly duplicated\n",
    "df_airports_data.select(\"iata_code\",\"iso_country\").groupBy(\"iata_code\",\"iso_country\")\\\n",
    "                        .count().where(col(\"count\") > 1).sort(col(\"count\").desc()).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small_airport</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            type  count\n",
       "0  small_airport     80"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since country code Brazil has 0 iata code replicated 80 times, lets explore what is happening in Brazil\n",
    "df_airports_data.where(col(\"iata_code\")==\"0\").groupBy(\"type\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata_code</th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHT</td>\n",
       "      <td>closed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMN</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDU</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DLR</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DZI</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GGC</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GVA</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HLA</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IST</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IZA</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JNB</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KCZ</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KMM</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KWB</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LMC</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LPE</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MPT</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MXR</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NWT</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>OHE</td>\n",
       "      <td>closed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PCO</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PRI</td>\n",
       "      <td>closed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>REQ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RMD</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RTI</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RZS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ULG</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>YTY</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ZRH</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ZRZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iata_code            type  count\n",
       "0        AHT          closed      2\n",
       "1        CMN   large_airport      2\n",
       "2        DDU   small_airport      2\n",
       "3        DLR   small_airport      2\n",
       "4        DZI   small_airport      2\n",
       "5        GGC   small_airport      2\n",
       "6        GVA   large_airport      2\n",
       "7        HLA  medium_airport      2\n",
       "8        IST   large_airport      2\n",
       "9        IZA  medium_airport      2\n",
       "10       JNB   large_airport      2\n",
       "11       KCZ  medium_airport      2\n",
       "12       KMM   small_airport      2\n",
       "13       KWB   small_airport      2\n",
       "14       LMC   small_airport      2\n",
       "15       LPE   small_airport      2\n",
       "16       MPT   small_airport      2\n",
       "17       MXR   small_airport      2\n",
       "18       NWT   small_airport      2\n",
       "19       OHE          closed      2\n",
       "20       PCO   small_airport      2\n",
       "21       PRI          closed      2\n",
       "22       REQ   small_airport      2\n",
       "23       RMD   small_airport      2\n",
       "24       RTI   small_airport      2\n",
       "25       RZS   small_airport      2\n",
       "26       ULG   small_airport      2\n",
       "27       YTY  medium_airport      2\n",
       "28       ZRH   large_airport      2\n",
       "29       ZRZ   small_airport      2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All the 0 code airports are small airports \n",
    "#Lets check the remaining airports and by types\n",
    "df_airports_data.select(\"iata_code\",\"type\").groupBy(\"iata_code\",\"type\")\\\n",
    "                        .count().where((col(\"count\") > 1) & (col(\"iata_code\") != '0')).sort(col(\"iata_code\")).limit(50).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There seems to be no direct relation of type of airports to duplication. Lets filter duplication on **type** of airports, filter out iata_code \"0\" since its all small airports and is not part of mapping in **i94PORT** codes, and also worth removing any airports that are closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airports_data = df_airports_data.filter((col(\"iata_code\")!=\"0\") & (col(\"type\") != \"closed\"))\\\n",
    "                            .dropDuplicates(subset=[\"iata_code\",\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8797"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the unique values for IATA_CODE\n",
    "df_airports_data.select(\"iata_code\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8803"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets compare it with total number of records\n",
    "df_airports_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata_code</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LHG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MRE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRM</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RCH</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>YMX</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iata_code  count\n",
       "0       LHG      2\n",
       "1       MRE      2\n",
       "2       PRM      2\n",
       "3       RCH      2\n",
       "4       SGL      2\n",
       "5       YMX      2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since still unique values are less that total rows, lets find the duplicated airports iata_codes\n",
    "df_airports_data.select(\"iata_code\").groupBy(\"iata_code\")\\\n",
    "                        .count().where(col(\"count\") > 1).sort(col(\"iata_code\")).limit(50).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YLRD</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Lightning Ridge Airport</td>\n",
       "      <td>540</td>\n",
       "      <td>OC</td>\n",
       "      <td>AU</td>\n",
       "      <td>AU-NSW</td>\n",
       "      <td>None</td>\n",
       "      <td>YLRD</td>\n",
       "      <td>LHG</td>\n",
       "      <td>None</td>\n",
       "      <td>147.98399353027344, -29.45669937133789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EFDD</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>LOAN OFFER</td>\n",
       "      <td>None</td>\n",
       "      <td>AS</td>\n",
       "      <td>AE</td>\n",
       "      <td>AE-DU</td>\n",
       "      <td>dubai</td>\n",
       "      <td>EFDD</td>\n",
       "      <td>LHG</td>\n",
       "      <td>NY33</td>\n",
       "      <td>0, 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KE-MRE</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Mara Lodges Airport</td>\n",
       "      <td>5706</td>\n",
       "      <td>AF</td>\n",
       "      <td>KE</td>\n",
       "      <td>KE-700</td>\n",
       "      <td>Mara Lodges</td>\n",
       "      <td>None</td>\n",
       "      <td>MRE</td>\n",
       "      <td>None</td>\n",
       "      <td>35.11130905151367, -1.1782759428024292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HKMS</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Mara Serena Lodge Airstrip</td>\n",
       "      <td>5200</td>\n",
       "      <td>AF</td>\n",
       "      <td>KE</td>\n",
       "      <td>KE-700</td>\n",
       "      <td>Masai Mara</td>\n",
       "      <td>HKMS</td>\n",
       "      <td>MRE</td>\n",
       "      <td>None</td>\n",
       "      <td>35.008057, -1.406111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CO-0039</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Proma Heliport</td>\n",
       "      <td>None</td>\n",
       "      <td>SA</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO-CUN</td>\n",
       "      <td>Bogota</td>\n",
       "      <td>None</td>\n",
       "      <td>PRM</td>\n",
       "      <td>None</td>\n",
       "      <td>-74.084444, 4.728056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LPPM</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Portimo Airport</td>\n",
       "      <td>5</td>\n",
       "      <td>EU</td>\n",
       "      <td>PT</td>\n",
       "      <td>PT-08</td>\n",
       "      <td>Portimo</td>\n",
       "      <td>LPPM</td>\n",
       "      <td>PRM</td>\n",
       "      <td>None</td>\n",
       "      <td>-8.58396, 37.1493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UY-0002</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Rocha Airport</td>\n",
       "      <td>91</td>\n",
       "      <td>SA</td>\n",
       "      <td>UY</td>\n",
       "      <td>UY-RO</td>\n",
       "      <td>Rocha</td>\n",
       "      <td>None</td>\n",
       "      <td>RCH</td>\n",
       "      <td>None</td>\n",
       "      <td>-54.27989959716797, -34.47809982299805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SKRH</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Almirante Padilla Airport</td>\n",
       "      <td>43</td>\n",
       "      <td>SA</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO-LAG</td>\n",
       "      <td>Riohacha</td>\n",
       "      <td>SKRH</td>\n",
       "      <td>RCH</td>\n",
       "      <td>RCH</td>\n",
       "      <td>-72.926, 11.5262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RPLS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Danilo Atienza Air Base</td>\n",
       "      <td>8</td>\n",
       "      <td>AS</td>\n",
       "      <td>PH</td>\n",
       "      <td>PH-CAV</td>\n",
       "      <td>Cavite City</td>\n",
       "      <td>RPLS</td>\n",
       "      <td>SGL</td>\n",
       "      <td>None</td>\n",
       "      <td>120.90399932861, 14.495400428772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SGL</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Danilo Atienza Air Base</td>\n",
       "      <td>None</td>\n",
       "      <td>AS</td>\n",
       "      <td>PH</td>\n",
       "      <td>PH-U-A</td>\n",
       "      <td>Cavite</td>\n",
       "      <td>RPLS</td>\n",
       "      <td>SGL</td>\n",
       "      <td>None</td>\n",
       "      <td>120.906987, 14.49562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>YMX</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>International Airport of Mirabel - Academy of ...</td>\n",
       "      <td>None</td>\n",
       "      <td>NA</td>\n",
       "      <td>CA</td>\n",
       "      <td>CA-QC</td>\n",
       "      <td>Mirabel</td>\n",
       "      <td>None</td>\n",
       "      <td>YMX</td>\n",
       "      <td>None</td>\n",
       "      <td>0, 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CYMX</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Montreal International (Mirabel) Airport</td>\n",
       "      <td>270</td>\n",
       "      <td>NA</td>\n",
       "      <td>CA</td>\n",
       "      <td>CA-QC</td>\n",
       "      <td>Montral</td>\n",
       "      <td>CYMX</td>\n",
       "      <td>YMX</td>\n",
       "      <td>None</td>\n",
       "      <td>-74.038696, 45.679501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ident            type  \\\n",
       "0      YLRD  medium_airport   \n",
       "1      EFDD   large_airport   \n",
       "2    KE-MRE   small_airport   \n",
       "3      HKMS  medium_airport   \n",
       "4   CO-0039        heliport   \n",
       "5      LPPM  medium_airport   \n",
       "6   UY-0002   small_airport   \n",
       "7      SKRH  medium_airport   \n",
       "8      RPLS   small_airport   \n",
       "9       SGL        heliport   \n",
       "10      YMX   large_airport   \n",
       "11     CYMX  medium_airport   \n",
       "\n",
       "                                                 name elevation_ft continent  \\\n",
       "0                             Lightning Ridge Airport          540        OC   \n",
       "1                                          LOAN OFFER         None        AS   \n",
       "2                                 Mara Lodges Airport         5706        AF   \n",
       "3                          Mara Serena Lodge Airstrip         5200        AF   \n",
       "4                                      Proma Heliport         None        SA   \n",
       "5                                   Portimo Airport            5        EU   \n",
       "6                                       Rocha Airport           91        SA   \n",
       "7                           Almirante Padilla Airport           43        SA   \n",
       "8                             Danilo Atienza Air Base            8        AS   \n",
       "9                             Danilo Atienza Air Base         None        AS   \n",
       "10  International Airport of Mirabel - Academy of ...         None        NA   \n",
       "11           Montreal International (Mirabel) Airport          270        NA   \n",
       "\n",
       "   iso_country iso_region municipality gps_code iata_code local_code  \\\n",
       "0           AU     AU-NSW         None     YLRD       LHG       None   \n",
       "1           AE      AE-DU        dubai     EFDD       LHG       NY33   \n",
       "2           KE     KE-700  Mara Lodges     None       MRE       None   \n",
       "3           KE     KE-700   Masai Mara     HKMS       MRE       None   \n",
       "4           CO     CO-CUN       Bogota     None       PRM       None   \n",
       "5           PT      PT-08    Portimo     LPPM       PRM       None   \n",
       "6           UY      UY-RO        Rocha     None       RCH       None   \n",
       "7           CO     CO-LAG     Riohacha     SKRH       RCH        RCH   \n",
       "8           PH     PH-CAV  Cavite City     RPLS       SGL       None   \n",
       "9           PH     PH-U-A       Cavite     RPLS       SGL       None   \n",
       "10          CA      CA-QC      Mirabel     None       YMX       None   \n",
       "11          CA      CA-QC    Montral     CYMX       YMX       None   \n",
       "\n",
       "                               coordinates  \n",
       "0   147.98399353027344, -29.45669937133789  \n",
       "1                                     0, 0  \n",
       "2   35.11130905151367, -1.1782759428024292  \n",
       "3                     35.008057, -1.406111  \n",
       "4                     -74.084444, 4.728056  \n",
       "5                        -8.58396, 37.1493  \n",
       "6   -54.27989959716797, -34.47809982299805  \n",
       "7                         -72.926, 11.5262  \n",
       "8         120.90399932861, 14.495400428772  \n",
       "9                     120.906987, 14.49562  \n",
       "10                                    0, 0  \n",
       "11                   -74.038696, 45.679501  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally lets break it down for each IATA code and see if there is any  reason for duplication\n",
    "df_airports_data.where(col(\"iata_code\").isin([\"MRE\",\"YMX\",\"SGL\",\"RCH\",\"PRM\",\"LHG\"])).orderBy(col(\"iata_code\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Since all these airports dont have a pattern for duplication and there IATA codes dont exist in i94PORT codes, it is better we filter these out\n",
    "df_airports_data = df_airports_data.filter(col(\"iata_code\").isin([\"MRE\",\"YMX\",\"SGL\",\"RCH\",\"PRM\",\"LHG\"]) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final check of unique values\n",
    "(df_airports_data.select(\"iata_code\").distinct().count()/df_airports_data.count())*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since there is 100 percent match to number of unique iata_codes to number of rows, we have achieved data integrity. We now do not need ident field, and that can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The column iso_region has hiphen, so we can clean and extract region code\n",
    "#Also lets extract the latitude and longitude from coordinates column, and convert them into float to be used for geo plotting and analytics\n",
    "df_airports_data = df_airports_data.withColumn(\"region_code\", to_split_extract_string(df_airports_data.iso_region,lit(\"-\"),lit(1)))\\\n",
    "                         .withColumn(\"latitude\", to_split_extract_float(df_airports_data.coordinates,lit(\",\"),lit(0),lit(\"float\")))\\\n",
    "                         .withColumn(\"longitude\", to_split_extract_float(df_airports_data.coordinates,lit(\",\"),lit(1),lit(\"float\")))\\\n",
    "                         .drop(\"coordinates\")\n",
    "\n",
    "#Finally subsetting the dataframe where iata_code will be the main identifier\n",
    "df_airports_data = df_airports_data.selectExpr(\"iata_code as id\",\\\n",
    "                                     \"type\",\\\n",
    "                                     \"name\",\\\n",
    "                                     \"elevation_ft\",\\\n",
    "                                     \"continent\",\\\n",
    "                                     \"iso_country\",\\\n",
    "                                     \"iso_region\",\\\n",
    "                                     \"municipality\",\\\n",
    "                                     \"gps_code\",\\\n",
    "                                     \"region_code\",\\\n",
    "                                     \"latitude\",\\\n",
    "                                     \"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>region_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BCA</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Gustavo Rizo Airport</td>\n",
       "      <td>26</td>\n",
       "      <td>NA</td>\n",
       "      <td>CU</td>\n",
       "      <td>CU-14</td>\n",
       "      <td>Baracoa</td>\n",
       "      <td>MUBA</td>\n",
       "      <td>14</td>\n",
       "      <td>-74.506203</td>\n",
       "      <td>20.365299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BGA</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Palonegro Airport</td>\n",
       "      <td>3897</td>\n",
       "      <td>SA</td>\n",
       "      <td>CO</td>\n",
       "      <td>CO-SAN</td>\n",
       "      <td>Bucaramanga</td>\n",
       "      <td>SKBG</td>\n",
       "      <td>SAN</td>\n",
       "      <td>-73.184799</td>\n",
       "      <td>7.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BWB</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Barrow Island Airport</td>\n",
       "      <td>26</td>\n",
       "      <td>OC</td>\n",
       "      <td>AU</td>\n",
       "      <td>AU-WA</td>\n",
       "      <td>None</td>\n",
       "      <td>YBWX</td>\n",
       "      <td>WA</td>\n",
       "      <td>115.405998</td>\n",
       "      <td>-20.864401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CFM</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Conklin (Leismer) Airport</td>\n",
       "      <td>1930</td>\n",
       "      <td>NA</td>\n",
       "      <td>CA</td>\n",
       "      <td>CA-AB</td>\n",
       "      <td>Conklin</td>\n",
       "      <td>CET2</td>\n",
       "      <td>AB</td>\n",
       "      <td>-111.278999</td>\n",
       "      <td>55.695301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CGY</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>Laguindingan Airport</td>\n",
       "      <td>190</td>\n",
       "      <td>AS</td>\n",
       "      <td>PH</td>\n",
       "      <td>PH-MSR</td>\n",
       "      <td>Cagayan de Oro City</td>\n",
       "      <td>RPMY</td>\n",
       "      <td>MSR</td>\n",
       "      <td>124.456497</td>\n",
       "      <td>8.612203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id            type                       name elevation_ft continent  \\\n",
       "0  BCA  medium_airport       Gustavo Rizo Airport           26        NA   \n",
       "1  BGA  medium_airport          Palonegro Airport         3897        SA   \n",
       "2  BWB   small_airport      Barrow Island Airport           26        OC   \n",
       "3  CFM   small_airport  Conklin (Leismer) Airport         1930        NA   \n",
       "4  CGY   large_airport       Laguindingan Airport          190        AS   \n",
       "\n",
       "  iso_country iso_region         municipality gps_code region_code  \\\n",
       "0          CU      CU-14              Baracoa     MUBA          14   \n",
       "1          CO     CO-SAN          Bucaramanga     SKBG         SAN   \n",
       "2          AU      AU-WA                 None     YBWX          WA   \n",
       "3          CA      CA-AB              Conklin     CET2          AB   \n",
       "4          PH     PH-MSR  Cagayan de Oro City     RPMY         MSR   \n",
       "\n",
       "     latitude  longitude  \n",
       "0  -74.506203  20.365299  \n",
       "1  -73.184799   7.126500  \n",
       "2  115.405998 -20.864401  \n",
       "3 -111.278999  55.695301  \n",
       "4  124.456497   8.612203  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the 5 rows of airport data extracted\n",
    "df_airports_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dates can provide more features which can then lead to insightful information, hence lets extract them and create more features and use them for analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Query to create unique dates from departure and arrival date of immigration fact table\n",
    "query = \"\"\"\n",
    "           SELECT distinct date_date FROM\n",
    "           (\n",
    "           SELECT arr_date AS date_date FROM  temp_date_table\n",
    "           UNION ALL\n",
    "           SELECT dept_date AS date_date FROM temp_date_table)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#running the query and creating a spark dataframe\n",
    "df_date_data = sc.sql(sqlQuery=query)\n",
    "\n",
    "#Lets create some date features such as month, day, week number,day of week,etc.\n",
    "df_date_data = df_date_data.select(\"date_date\",\\\n",
    "               dayofmonth(\"date_date\").alias(\"day\"),\\\n",
    "               weekofyear(\"date_date\").alias(\"week\"),\\\n",
    "               month(\"date_date\").alias(\"month\"),\\\n",
    "               date_format(\"date_date\", 'MMMM').alias(\"month_name\"),\\\n",
    "               year(\"date_date\").alias(\"year\"),\\\n",
    "               dayofweek(\"date_date\").alias(\"day_of_week\"),\\\n",
    "               date_format(\"date_Date\", 'E').alias(\"day_of_week_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_date</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>month_name</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_week_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>August</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-05-31</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>May</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>Tue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>August</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-10</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>April</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>April</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>Sat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_date  day  week  month month_name  year  day_of_week day_of_week_name\n",
       "0 2016-08-21   21    33      8     August  2016            1              Sun\n",
       "1 2016-05-31   31    22      5        May  2016            3              Tue\n",
       "2 2016-08-14   14    32      8     August  2016            1              Sun\n",
       "3 2016-04-10   10    14      4      April  2016            1              Sun\n",
       "4 2016-04-30   30    17      4      April  2016            7              Sat"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the 5 rows of dates data with new features\n",
    "df_date_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As explored above with pandas, temperature data has many records almost dating back to 1743, however since immigration data is mostly 2016 we can filter out all data before 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Loading the climate/temperature data in spark\n",
    "climate_file_path = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature_data = spark.read.format(\"csv\").option(\"header\",True).load(climate_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets see the data types\n",
    "df_temperature_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Since date is a string, lets extract it to date column. Also worth adding a month feature to enable better insights and \n",
    "# worth filtering out data before 2000\n",
    "df_temperature_data = df_temperature_data.withColumn(\"dt\",to_date(\"dt\",'yyyy-MM-dd'))\\\n",
    "                                         .withColumn(\"month\",date_format(\"dt\",\"MMMM\"))\\\n",
    "                                         .filter(col(\"dt\") >= \"2000-01-01\")\n",
    "                                    \n",
    "                                    \n",
    "\n",
    "#The latitude and longitude are in string format, which will make analytics and geo plotting difficult, so lets cast them to float\n",
    "df_temperature_data = df_temperature_data.withColumn(\"Latitude\",cast_lat_lon(df_temperature_data.Latitude))\\\n",
    "                                         .withColumn(\"Longitude\",cast_lat_lon(df_temperature_data.Longitude))\\\n",
    "                                         .withColumn(\"Country\", upper(df_temperature_data.Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_date</th>\n",
       "      <th>min_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_date    min_date\n",
       "0  2013-09-01  2000-01-01"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting min and max date of temperature data\n",
    "df_temperature_data.selectExpr(\"max(dt) as max_date\", \"min(dt) as min_date\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since cities can have similar names across countries and sometimes states, and is difficult to establish a direct relationship with cities in immigration data, lets just focus on countries temperature data. Also dates between immigration and climate dont coincide completely, as temperature data have missing 3 years of data, so we can just use average on monthly countries temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lets drop cities, dt and AverageTemperatureUncertainty \n",
    "\n",
    "df_temperature_data = df_temperature_data.drop(\"AverageTemperatureUncertainty\",\"City\",\"dt\")\n",
    "\n",
    "\n",
    "#Lets first group on country and month\n",
    "df_temperature_data = df_temperature_data.groupby(\"Country\", \"month\")\\\n",
    "                   .agg(mean(\"AverageTemperature\").alias(\"AverageTemperature\"),\\\n",
    "                        mean(\"Latitude\").alias(\"Latitude\"),\\\n",
    "                        mean(\"Longitude\").alias(\"Longitude\"))\n",
    "\n",
    "#Since countries will be duplicated and country and month make unique combination , we can just pivot on month, and have unique rows based\n",
    "#on country\n",
    "\n",
    "#First grouping on country\n",
    "df_temperature_lat_lon = df_temperature_data.groupby(\"Country\")\\\n",
    "                        .agg(first(\"Latitude\").alias(\"Latitude\"),\\\n",
    "                             first(\"Longitude\").alias(\"Longitude\"))\n",
    "\n",
    "#Pivoting on months and extracting averaget temperatures\n",
    "df_temperature_data = df_temperature_data.groupby(\"Country\")\\\n",
    "                        .pivot(\"Month\")\\\n",
    "                        .agg(mean(\"AverageTemperature\").alias(\"AverageTemperature\"))\n",
    "\n",
    "#Finally joining back to get the final data \n",
    "df_temperature_data = df_temperature_data.join(df_temperature_lat_lon, on=\"Country\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can use i94cit and i94res mappings to join with temperature data, and put it all under regions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Getting the i94cit and res mappings\n",
    "df_regions_data = spark.createDataFrame(list(map(list, cit_and_res_codes.items())),\n",
    "                                               [\"cit_res_code\",\"cit_res_name\"])\n",
    "\n",
    "#Creating a column to join to temperature table\n",
    "df_regions_data = df_regions_data.withColumn(\"Country\", df_regions_data.cit_res_name)\n",
    "\n",
    "#Joining back to temperature data and adding column names\n",
    "df_regions_data = df_regions_data.join(df_temperature_data, on=\"Country\", how=\"left\")\n",
    "df_regions_data = df_regions_data.selectExpr(\"cit_res_code\",\\\n",
    "                                             \"cit_res_name\",\\\n",
    "                                             \"Latitude as latitude\",\\\n",
    "                                             \"Longitude as longitude\",\\\n",
    "                                             \"January as temp_january\",\\\n",
    "                                             \"February as temp_february\",\\\n",
    "                                             \"March as temp_march\",\\\n",
    "                                             \"April as temp_april\",\\\n",
    "                                             \"May as temp_may\",\\\n",
    "                                             \"June as temp_june\",\\\n",
    "                                             \"July as temp_july\",\\\n",
    "                                             \"August as temp_august\",\\\n",
    "                                             \"September as temp_september\",\\\n",
    "                                             \"October as temp_october\",\\\n",
    "                                             \"November as temp_november\",\\\n",
    "                                             \"December as temp_december\")\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cit_res_code</th>\n",
       "      <th>cit_res_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>temp_january</th>\n",
       "      <th>temp_february</th>\n",
       "      <th>temp_march</th>\n",
       "      <th>temp_april</th>\n",
       "      <th>temp_may</th>\n",
       "      <th>temp_june</th>\n",
       "      <th>temp_july</th>\n",
       "      <th>temp_august</th>\n",
       "      <th>temp_september</th>\n",
       "      <th>temp_october</th>\n",
       "      <th>temp_november</th>\n",
       "      <th>temp_december</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151</td>\n",
       "      <td>ARMENIA</td>\n",
       "      <td>40.990002</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>-3.168214</td>\n",
       "      <td>-1.168286</td>\n",
       "      <td>3.802357</td>\n",
       "      <td>9.074071</td>\n",
       "      <td>13.886571</td>\n",
       "      <td>18.552286</td>\n",
       "      <td>21.878857</td>\n",
       "      <td>22.051214</td>\n",
       "      <td>17.523769</td>\n",
       "      <td>11.301154</td>\n",
       "      <td>4.420923</td>\n",
       "      <td>-1.065769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>512</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>24.920000</td>\n",
       "      <td>-78.029999</td>\n",
       "      <td>21.002071</td>\n",
       "      <td>22.173214</td>\n",
       "      <td>23.392714</td>\n",
       "      <td>24.814500</td>\n",
       "      <td>26.678000</td>\n",
       "      <td>28.324500</td>\n",
       "      <td>29.111214</td>\n",
       "      <td>29.229000</td>\n",
       "      <td>28.370357</td>\n",
       "      <td>26.490923</td>\n",
       "      <td>23.826154</td>\n",
       "      <td>22.132385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739</td>\n",
       "      <td>INVALID: DRONNING MAUD LAND (ANTARCTICA-NORWAY)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>373</td>\n",
       "      <td>SOUTH AFRICA</td>\n",
       "      <td>-28.129400</td>\n",
       "      <td>27.331600</td>\n",
       "      <td>21.170151</td>\n",
       "      <td>21.256719</td>\n",
       "      <td>19.966431</td>\n",
       "      <td>16.898169</td>\n",
       "      <td>13.993247</td>\n",
       "      <td>11.345843</td>\n",
       "      <td>10.889649</td>\n",
       "      <td>13.488910</td>\n",
       "      <td>16.554420</td>\n",
       "      <td>18.634994</td>\n",
       "      <td>19.675094</td>\n",
       "      <td>20.693595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>914</td>\n",
       "      <td>No Country Code (914)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cit_res_code                                     cit_res_name   latitude  \\\n",
       "0           151                                          ARMENIA  40.990002   \n",
       "1           512                                          BAHAMAS  24.920000   \n",
       "2           739  INVALID: DRONNING MAUD LAND (ANTARCTICA-NORWAY)        NaN   \n",
       "3           373                                     SOUTH AFRICA -28.129400   \n",
       "4           914                            No Country Code (914)        NaN   \n",
       "\n",
       "   longitude  temp_january  temp_february  temp_march  temp_april   temp_may  \\\n",
       "0  44.730000     -3.168214      -1.168286    3.802357    9.074071  13.886571   \n",
       "1 -78.029999     21.002071      22.173214   23.392714   24.814500  26.678000   \n",
       "2        NaN           NaN            NaN         NaN         NaN        NaN   \n",
       "3  27.331600     21.170151      21.256719   19.966431   16.898169  13.993247   \n",
       "4        NaN           NaN            NaN         NaN         NaN        NaN   \n",
       "\n",
       "   temp_june  temp_july  temp_august  temp_september  temp_october  \\\n",
       "0  18.552286  21.878857    22.051214       17.523769     11.301154   \n",
       "1  28.324500  29.111214    29.229000       28.370357     26.490923   \n",
       "2        NaN        NaN          NaN             NaN           NaN   \n",
       "3  11.345843  10.889649    13.488910       16.554420     18.634994   \n",
       "4        NaN        NaN          NaN             NaN           NaN   \n",
       "\n",
       "   temp_november  temp_december  \n",
       "0       4.420923      -1.065769  \n",
       "1      23.826154      22.132385  \n",
       "2            NaN            NaN  \n",
       "3      19.675094      20.693595  \n",
       "4            NaN            NaN  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the 5 rows of the data\n",
    "df_regions_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Exploring the demographics data above using pandas , the column with most unique values was Count. Lets explore if there is duplication in other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Importing the demographics data\n",
    "df_demographics_data = spark.read.csv(\"us-cities-demographics.csv\", header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State Median Age Male Population  \\\n",
       "0     Silver Spring       Maryland       33.8           40601   \n",
       "1            Quincy  Massachusetts       41.0           44129   \n",
       "2            Hoover        Alabama       38.5           38040   \n",
       "3  Rancho Cucamonga     California       34.5           88127   \n",
       "4            Newark     New Jersey       34.6          138040   \n",
       "\n",
       "  Female Population Total Population Number of Veterans Foreign-born  \\\n",
       "0             41862            82463               1562        30908   \n",
       "1             49500            93629               4147        32935   \n",
       "2             46799            84839               4819         8229   \n",
       "3             87105           175232               5821        33878   \n",
       "4            143873           281913               5829        86253   \n",
       "\n",
       "  Average Household Size State Code                       Race  Count  \n",
       "0                    2.6         MD         Hispanic or Latino  25924  \n",
       "1                   2.39         MA                      White  58723  \n",
       "2                   2.58         AL                      Asian   4759  \n",
       "3                   3.18         CA  Black or African-American  24437  \n",
       "4                   2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing the 5 rows\n",
    "df_demographics_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>White</td>\n",
       "      <td>37756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>21330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Asian</td>\n",
       "      <td>8841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City     State Median Age Male Population Female Population  \\\n",
       "0  Silver Spring  Maryland       33.8           40601             41862   \n",
       "1  Silver Spring  Maryland       33.8           40601             41862   \n",
       "2  Silver Spring  Maryland       33.8           40601             41862   \n",
       "3  Silver Spring  Maryland       33.8           40601             41862   \n",
       "4  Silver Spring  Maryland       33.8           40601             41862   \n",
       "\n",
       "  Total Population Number of Veterans Foreign-born Average Household Size  \\\n",
       "0            82463               1562        30908                    2.6   \n",
       "1            82463               1562        30908                    2.6   \n",
       "2            82463               1562        30908                    2.6   \n",
       "3            82463               1562        30908                    2.6   \n",
       "4            82463               1562        30908                    2.6   \n",
       "\n",
       "  State Code                               Race  Count  \n",
       "0         MD                 Hispanic or Latino  25924  \n",
       "1         MD                              White  37756  \n",
       "2         MD          Black or African-American  21330  \n",
       "3         MD  American Indian and Alaska Native   1084  \n",
       "4         MD                              Asian   8841  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets filter by a city name\n",
    "df_demographics_data.where(col(\"City\") == \"Silver Spring\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since data seems duplicated for majority of columns except for **Race** and **Count** column, we may need to separate the data set, \n",
    "removed duplicates and join back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FMY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RIV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OAK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FAR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DET</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>REN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TAC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NSV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SFR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JER</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BHX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SAV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HAR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CNJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WIL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>None</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CAK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MOB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NWH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TUC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LVG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>VAN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ERI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TOL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BUF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>POM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>CHL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>POO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>ELP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>SAC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>ATL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>PHI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>PRO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>OMA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>SPO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>BIL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>BEA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>WPB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>SYR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PON</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>LOS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>COS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>HLG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>HOU</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>BEL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>YUM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ONT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>SNJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>ROC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>CHI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>ANC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>DSM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>ORL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>SLC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>NYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>MIL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     code  count\n",
       "0     FMY      1\n",
       "1     SNA      1\n",
       "2     GRB      1\n",
       "3     EVE      1\n",
       "4     RIV      1\n",
       "5     OAK      1\n",
       "6     FAR      1\n",
       "7     DET      1\n",
       "8     REN      1\n",
       "9     TAC      1\n",
       "10    NSV      1\n",
       "11    SFR      1\n",
       "12    JER      1\n",
       "13    BHX      1\n",
       "14    SAV      1\n",
       "15    HAR      1\n",
       "16    CNJ      1\n",
       "17    WIL      1\n",
       "18   None    483\n",
       "19    CAK      1\n",
       "20    MOB      1\n",
       "21    NWH      1\n",
       "22    TUC      1\n",
       "23    LVG      1\n",
       "24    VAN      1\n",
       "25    ERI      1\n",
       "26    TOL      1\n",
       "27    BUF      1\n",
       "28    POM      1\n",
       "29    BAL      1\n",
       "..    ...    ...\n",
       "86    CHL      1\n",
       "87    POO      1\n",
       "88    ELP      1\n",
       "89    SAC      1\n",
       "90    ATL      1\n",
       "91    PHI      1\n",
       "92    PRO      1\n",
       "93    OMA      1\n",
       "94    SPO      1\n",
       "95    BIL      1\n",
       "96    BEA      1\n",
       "97    WPB      1\n",
       "98    SYR      1\n",
       "99    PON      1\n",
       "100   LOS      1\n",
       "101   COS      1\n",
       "102   HLG      1\n",
       "103   HOU      1\n",
       "104   BEL      1\n",
       "105   YUM      1\n",
       "106   ONT      1\n",
       "107   SNJ      1\n",
       "108   ROC      1\n",
       "109   CHI      1\n",
       "110   ANC      1\n",
       "111   DSM      1\n",
       "112   ORL      1\n",
       "113   SLC      1\n",
       "114   NYC      1\n",
       "115   MIL      1\n",
       "\n",
       "[116 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets explore if we will join with i94PORT, and how many matches we will get\n",
    "df_demographics_data.dropDuplicates(subset=[\"City\",\"State\",\"State Code\"])\\\n",
    "                    .withColumn(\"City\", upper(col(\"City\")))\\\n",
    "                    .withColumn(\"port_name\", concat(col(\"City\"),lit(\", \"),col(\"State Code\")))\\\n",
    "                    .join(df_ports_codes, on=\"port_name\", how=\"left\")\\\n",
    "                    .groupBy(\"code\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since majority of matches are None (483), its worth getting rid of city, so we can join with immigration on **State Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Subsetting the data without race and count, and aggregating after removing duplicates\n",
    "df_demographics_state_city_data = df_demographics_data.dropDuplicates(subset=[\"City\",\"State\",\"State Code\"])\\\n",
    "                                                      .groupby(\"State Code\",\"State\")\\\n",
    "                                                      .agg(mean(\"Median Age\").alias(\"median_age\"),\\\n",
    "                                                            Sum(\"Male Population\").alias(\"male_population\"),\\\n",
    "                                                            Sum(\"Female Population\").alias(\"female_population\"),\\\n",
    "                                                            Sum(\"Total Population\").alias(\"total_population\"),\\\n",
    "                                                            Sum(\"Number of Veterans\").alias(\"veterans_population\"),\\\n",
    "                                                            Sum(\"Foreign-born\").alias(\"foreign_born_population\"),\\\n",
    "                                                            mean(\"Average Household Size\").alias(\"household_size_avg\"))\n",
    "\n",
    "#Subsetting out the race data and splitting race to it individual columns, by doing a pivot\n",
    "df_race_state_city_data = df_demographics_data.select(\"State Code\", \"Race\", \"Count\")\\\n",
    "                                   .groupby(\"State Code\")\\\n",
    "                                   .pivot(\"Race\")\\\n",
    "                                   .agg(Sum(\"Count\"))\n",
    "\n",
    "#Once the data is transformed and normalised, can join and bring back together\n",
    "df_demographics_data = df_demographics_state_city_data.join(df_race_state_city_data,\\\n",
    "                                                 on=\"State Code\",\\\n",
    "                                                 how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State Code</th>\n",
       "      <th>State</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>veterans_population</th>\n",
       "      <th>foreign_born_population</th>\n",
       "      <th>household_size_avg</th>\n",
       "      <th>American Indian and Alaska Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black or African-American</th>\n",
       "      <th>Hispanic or Latino</th>\n",
       "      <th>White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AZ</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>35.037500</td>\n",
       "      <td>2227455.0</td>\n",
       "      <td>2272087.0</td>\n",
       "      <td>4499542.0</td>\n",
       "      <td>264505.0</td>\n",
       "      <td>682313.0</td>\n",
       "      <td>2.774375</td>\n",
       "      <td>129708.0</td>\n",
       "      <td>229183.0</td>\n",
       "      <td>296222.0</td>\n",
       "      <td>1508157.0</td>\n",
       "      <td>3591611.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SC</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>34.180000</td>\n",
       "      <td>260944.0</td>\n",
       "      <td>272713.0</td>\n",
       "      <td>533657.0</td>\n",
       "      <td>33463.0</td>\n",
       "      <td>27744.0</td>\n",
       "      <td>2.472000</td>\n",
       "      <td>3705.0</td>\n",
       "      <td>13355.0</td>\n",
       "      <td>175064.0</td>\n",
       "      <td>29863.0</td>\n",
       "      <td>343764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LA</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>34.625000</td>\n",
       "      <td>626998.0</td>\n",
       "      <td>673597.0</td>\n",
       "      <td>1300595.0</td>\n",
       "      <td>69771.0</td>\n",
       "      <td>83419.0</td>\n",
       "      <td>2.465000</td>\n",
       "      <td>8263.0</td>\n",
       "      <td>38739.0</td>\n",
       "      <td>602377.0</td>\n",
       "      <td>87133.0</td>\n",
       "      <td>654578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MN</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>35.618182</td>\n",
       "      <td>702157.0</td>\n",
       "      <td>720246.0</td>\n",
       "      <td>1422403.0</td>\n",
       "      <td>64894.0</td>\n",
       "      <td>215873.0</td>\n",
       "      <td>2.500909</td>\n",
       "      <td>25242.0</td>\n",
       "      <td>151544.0</td>\n",
       "      <td>216731.0</td>\n",
       "      <td>103229.0</td>\n",
       "      <td>1050239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NJ</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>35.125000</td>\n",
       "      <td>705736.0</td>\n",
       "      <td>723172.0</td>\n",
       "      <td>1428908.0</td>\n",
       "      <td>30195.0</td>\n",
       "      <td>477028.0</td>\n",
       "      <td>2.965833</td>\n",
       "      <td>11350.0</td>\n",
       "      <td>116844.0</td>\n",
       "      <td>452202.0</td>\n",
       "      <td>600437.0</td>\n",
       "      <td>615083.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State Code           State  median_age  male_population  female_population  \\\n",
       "0         AZ         Arizona   35.037500        2227455.0          2272087.0   \n",
       "1         SC  South Carolina   34.180000         260944.0           272713.0   \n",
       "2         LA       Louisiana   34.625000         626998.0           673597.0   \n",
       "3         MN       Minnesota   35.618182         702157.0           720246.0   \n",
       "4         NJ      New Jersey   35.125000         705736.0           723172.0   \n",
       "\n",
       "   total_population  veterans_population  foreign_born_population  \\\n",
       "0         4499542.0             264505.0                 682313.0   \n",
       "1          533657.0              33463.0                  27744.0   \n",
       "2         1300595.0              69771.0                  83419.0   \n",
       "3         1422403.0              64894.0                 215873.0   \n",
       "4         1428908.0              30195.0                 477028.0   \n",
       "\n",
       "   household_size_avg  American Indian and Alaska Native     Asian  \\\n",
       "0            2.774375                           129708.0  229183.0   \n",
       "1            2.472000                             3705.0   13355.0   \n",
       "2            2.465000                             8263.0   38739.0   \n",
       "3            2.500909                            25242.0  151544.0   \n",
       "4            2.965833                            11350.0  116844.0   \n",
       "\n",
       "   Black or African-American  Hispanic or Latino      White  \n",
       "0                   296222.0           1508157.0  3591611.0  \n",
       "1                   175064.0             29863.0   343764.0  \n",
       "2                   602377.0             87133.0   654578.0  \n",
       "3                   216731.0            103229.0  1050239.0  \n",
       "4                   452202.0            600437.0   615083.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets view the data again\n",
    "df_demographics_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets count the rows and see if many duplicates has been removed, rows shouldnt exist more than 49\n",
    "df_demographics_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating the alias for few columns in demographics\n",
    "df_demographics_data = df_demographics_data.select(col(\"State Code\").alias(\"state_code\"),\\\n",
    "                                col(\"State\").alias(\"state_name\"),\\\n",
    "                                col(\"median_age\"),\\\n",
    "                                col(\"male_population\"),\\\n",
    "                                col(\"female_population\"),\\\n",
    "                                col(\"total_population\"),\\\n",
    "                                col(\"veterans_population\"),\\\n",
    "                                col(\"foreign_born_population\"),\\\n",
    "                                col(\"household_size_avg\"),\\\n",
    "                                col(\"American Indian and Alaska Native\")\\\n",
    "                                .alias(\"american_indian_and_alaskan_native_population\"),\\\n",
    "                                col(\"Asian\").alias(\"asian_population\"),\\\n",
    "                                col(\"Black or African-American\")\\\n",
    "                                .alias(\"black_or_african_american_populaton\"),\\\n",
    "                                col(\"Hispanic or Latino\")\\\n",
    "                                .alias(\"hispanic_latino_population\"),\\\n",
    "                                col(\"White\").alias(\"white_population\")\\\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>state_name</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>veterans_population</th>\n",
       "      <th>foreign_born_population</th>\n",
       "      <th>household_size_avg</th>\n",
       "      <th>american_indian_and_alaskan_native_population</th>\n",
       "      <th>asian_population</th>\n",
       "      <th>black_or_african_american_populaton</th>\n",
       "      <th>hispanic_latino_population</th>\n",
       "      <th>white_population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AZ</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>35.037500</td>\n",
       "      <td>2227455.0</td>\n",
       "      <td>2272087.0</td>\n",
       "      <td>4499542.0</td>\n",
       "      <td>264505.0</td>\n",
       "      <td>682313.0</td>\n",
       "      <td>2.774375</td>\n",
       "      <td>129708.0</td>\n",
       "      <td>229183.0</td>\n",
       "      <td>296222.0</td>\n",
       "      <td>1508157.0</td>\n",
       "      <td>3591611.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SC</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>34.180000</td>\n",
       "      <td>260944.0</td>\n",
       "      <td>272713.0</td>\n",
       "      <td>533657.0</td>\n",
       "      <td>33463.0</td>\n",
       "      <td>27744.0</td>\n",
       "      <td>2.472000</td>\n",
       "      <td>3705.0</td>\n",
       "      <td>13355.0</td>\n",
       "      <td>175064.0</td>\n",
       "      <td>29863.0</td>\n",
       "      <td>343764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LA</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>34.625000</td>\n",
       "      <td>626998.0</td>\n",
       "      <td>673597.0</td>\n",
       "      <td>1300595.0</td>\n",
       "      <td>69771.0</td>\n",
       "      <td>83419.0</td>\n",
       "      <td>2.465000</td>\n",
       "      <td>8263.0</td>\n",
       "      <td>38739.0</td>\n",
       "      <td>602377.0</td>\n",
       "      <td>87133.0</td>\n",
       "      <td>654578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MN</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>35.618182</td>\n",
       "      <td>702157.0</td>\n",
       "      <td>720246.0</td>\n",
       "      <td>1422403.0</td>\n",
       "      <td>64894.0</td>\n",
       "      <td>215873.0</td>\n",
       "      <td>2.500909</td>\n",
       "      <td>25242.0</td>\n",
       "      <td>151544.0</td>\n",
       "      <td>216731.0</td>\n",
       "      <td>103229.0</td>\n",
       "      <td>1050239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NJ</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>35.125000</td>\n",
       "      <td>705736.0</td>\n",
       "      <td>723172.0</td>\n",
       "      <td>1428908.0</td>\n",
       "      <td>30195.0</td>\n",
       "      <td>477028.0</td>\n",
       "      <td>2.965833</td>\n",
       "      <td>11350.0</td>\n",
       "      <td>116844.0</td>\n",
       "      <td>452202.0</td>\n",
       "      <td>600437.0</td>\n",
       "      <td>615083.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_code      state_name  median_age  male_population  female_population  \\\n",
       "0         AZ         Arizona   35.037500        2227455.0          2272087.0   \n",
       "1         SC  South Carolina   34.180000         260944.0           272713.0   \n",
       "2         LA       Louisiana   34.625000         626998.0           673597.0   \n",
       "3         MN       Minnesota   35.618182         702157.0           720246.0   \n",
       "4         NJ      New Jersey   35.125000         705736.0           723172.0   \n",
       "\n",
       "   total_population  veterans_population  foreign_born_population  \\\n",
       "0         4499542.0             264505.0                 682313.0   \n",
       "1          533657.0              33463.0                  27744.0   \n",
       "2         1300595.0              69771.0                  83419.0   \n",
       "3         1422403.0              64894.0                 215873.0   \n",
       "4         1428908.0              30195.0                 477028.0   \n",
       "\n",
       "   household_size_avg  american_indian_and_alaskan_native_population  \\\n",
       "0            2.774375                                       129708.0   \n",
       "1            2.472000                                         3705.0   \n",
       "2            2.465000                                         8263.0   \n",
       "3            2.500909                                        25242.0   \n",
       "4            2.965833                                        11350.0   \n",
       "\n",
       "   asian_population  black_or_african_american_populaton  \\\n",
       "0          229183.0                             296222.0   \n",
       "1           13355.0                             175064.0   \n",
       "2           38739.0                             602377.0   \n",
       "3          151544.0                             216731.0   \n",
       "4          116844.0                             452202.0   \n",
       "\n",
       "   hispanic_latino_population  white_population  \n",
       "0                   1508157.0         3591611.0  \n",
       "1                     29863.0          343764.0  \n",
       "2                     87133.0          654578.0  \n",
       "3                    103229.0         1050239.0  \n",
       "4                    600437.0          615083.0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing the 5 rows of the demographics data\n",
    "df_demographics_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Immigration Analytical Engine Data Model\n",
    "The analytical database will levereage the star schema design for query optimisation with immigration data held in fact table and others will be dimension tables. If we will normalise the data more, it may become slow for analysis. It is recommended practice for analytical databases (OLAP) to utilise star schema design with one or few fact tables and multiple dimension tables, to ensure fast query results and reduce joins, especially when Business Analysts and Data Science teams working for the government, is the key audience in for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![title](img/DataModel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps are in the etl.py. Its running the etl.py on EMR cluster on AWS will generate data and load it into S3 which can then be consumed by Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The code is in the etl.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The data quality checks that will be performed include are:\n",
    " * Ensure that identity/primary key on fact table and dimension table is unique, and doesnt have nulls\n",
    " * The immigration fact table date has date types, and dimension date table has date type\n",
    " * The total number of rows are greater than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_unique_keys(df,table_name=\"\", primary_key=\"\"):\n",
    "    \"\"\"\n",
    "    Function to check table primary key is unique in nature by comparing\n",
    "    distinct number of primary key values with total number of rows, a\n",
    "    value error is raised \n",
    "    \"\"\"\n",
    "    num_rows = df.count()\n",
    "    num_unique_identifier_rows = df.dropDuplicates(subset=[primary_key]).count()\n",
    "    if num_rows == num_unique_identifier_rows:\n",
    "        print(f\"{table_name} has unique rows, and primary key constraint is not violated\")\n",
    "    else:\n",
    "        raise ValueError(f\"{table_name} uniqueness violated , duplicated data detected\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ensure_no_nulls(df, column=\"\"):\n",
    "    \"\"\"\n",
    "    Function to check for null counts on spark dataframe columns, \n",
    "    a value error is raised if column has null values\n",
    "    \"\"\"\n",
    "    null_counts = df.filter(f\"{column} is NULL\").count()\n",
    "    if null_counts == 0:\n",
    "        print(f\"{column} doesnt have any nulls\")\n",
    "    else:\n",
    "        raise ValueError(f\"{column} violated the null constraint, cannot contain nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_data_type(df, column, datatype):\n",
    "    \"\"\"\n",
    "    Function to check datatype matches for spark dataframe.\n",
    "    If mismatch is detected then value error is raised, and if column\n",
    "    is not found then key error is raised\n",
    "    \"\"\"\n",
    "    for _ in df.schema:\n",
    "        if _.name == column:\n",
    "            if str(_.dataType) == datatype:\n",
    "                print(f\"datatype match for column {column} having {datatype} values\")\n",
    "                break\n",
    "            else:\n",
    "                raise ValueError(f\"datatype mismatch detected for column {column}. Expected {datatype} but got {_.dataType}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_greater_that_zero(df):\n",
    "    \"\"\"\n",
    "    Function to perform greater than 0 rows check for spark df.\n",
    "    If check fails a value error is raised\n",
    "    \"\"\"\n",
    "    if df.count() > 0:\n",
    "        print(f\"Greater than 0 test passed for the table\")\n",
    "    else:\n",
    "        raise ValueError(f\"Table has 0 rows, data may not have loaded correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def match_source_input(df_input, df_output):\n",
    "    \"\"\"\n",
    "    Function to check if data pushed to a location matches \n",
    "    data before pushing, to ensure data completeness, else\n",
    "    value error is raised\n",
    "    \"\"\"\n",
    "    if df_input.count() == df_output.count():\n",
    "        print(f\"Data pushed has complete data\")\n",
    "    else:\n",
    "        raise ValueError(f\"Data at source doesnt match data at destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_quality_checks():\n",
    "    \"\"\"\n",
    "    Main function for running quality checks on the fact table and all\n",
    "    dimension tables\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Running quality test for immigration_fct table\")\n",
    "    check_unique_keys(df_immigration_data, \"immigration_fct\", \"cicid\")\n",
    "    ensure_no_nulls(df_immigration_data, \"cicid\")\n",
    "    check_data_type(df_immigration_data, \"dept_date\", \"TimestampType\")\n",
    "    check_data_type(df_immigration_data, \"arr_date\", \"TimestampType\")\n",
    "    check_greater_that_zero(df_immigration_data)\n",
    "\n",
    "    print(\"\\nRunning quality test for airports_dm table\")\n",
    "    check_unique_keys(df_airports_data, \"airports_dm\", \"id\")\n",
    "    ensure_no_nulls(df_airports_data, \"id\")\n",
    "    check_greater_that_zero(df_airports_data)\n",
    "\n",
    "    print(\"\\nRunning quality test for regions_dm table\")\n",
    "    check_unique_keys(df_regions_data, \"regions_dm\", \"cit_res_code\")\n",
    "    ensure_no_nulls(df_regions_data, \"cit_res_code\")\n",
    "    check_greater_that_zero(df_regions_data)\n",
    "    \n",
    "    print(\"\\nRunning quality test for modes_dm table\")\n",
    "    check_unique_keys(df_mode_codes, \"modes_dm\", \"code\")\n",
    "    ensure_no_nulls(df_mode_codes,  \"code\")\n",
    "    check_greater_that_zero(df_mode_codes)\n",
    "\n",
    "    print(\"\\nRunning quality test for immi_dates_dm table\")\n",
    "    check_unique_keys(df_date_data, \"immi_dates_dm\", \"date_date\")\n",
    "    ensure_no_nulls(df_date_data, \"date_date\")\n",
    "    check_data_type(df_date_data, \"date_date\", \"TimestampType\")\n",
    "    check_greater_that_zero(df_date_data)\n",
    "\n",
    "    print(\"\\nRunning quality test for ports_dm table\")\n",
    "    check_unique_keys(df_ports_codes, \"ports_dm\", \"code\")\n",
    "    ensure_no_nulls(df_ports_codes, \"code\")\n",
    "    check_greater_that_zero(df_ports_codes)\n",
    "\n",
    "    print(\"\\nRunning quality test for visas_dm table\")\n",
    "    check_unique_keys(df_visa_codes, \"visas_dm\", \"code\")\n",
    "    ensure_no_nulls(df_visa_codes, \"code\")\n",
    "    check_greater_that_zero(df_visa_codes)\n",
    "\n",
    "    print(\"\\nRunning quality test for airlines_dm table\")\n",
    "    check_unique_keys(df_flights_data, \"airlines_dm\", \"flight_id\")\n",
    "    ensure_no_nulls(df_flights_data, \"flight_id\")\n",
    "    check_greater_that_zero(df_flights_data)\n",
    "\n",
    "    print(\"\\nRunning quality test for demographics_dm table\")\n",
    "    check_unique_keys(df_demographics_data, \"demographics_dm\", \"state_code\")\n",
    "    ensure_no_nulls(df_demographics_data, \"state_code\")\n",
    "    check_greater_that_zero(df_demographics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running quality test for immigration_fct table\n",
      "immigration_fct has unique rows, and primary key constraint is not violated\n",
      "cicid doesnt have any nulls\n",
      "datatype match for column dept_date having TimestampType values\n",
      "datatype match for column arr_date having TimestampType values\n",
      "Greater than 0 test passed for the table\n",
      "\n",
      "Running quality test for airports_dm table\n",
      "airports_dm has unique rows, and primary key constraint is not violated\n",
      "id doesnt have any nulls\n",
      "Greater than 0 test passed for the table\n",
      "\n",
      "Running quality test for regions_dm table\n",
      "regions_dm has unique rows, and primary key constraint is not violated\n",
      "cit_res_code doesnt have any nulls\n",
      "Greater than 0 test passed for the table\n",
      "\n",
      "Running quality test for modes_dm table\n",
      "modes_dm has unique rows, and primary key constraint is not violated\n",
      "code doesnt have any nulls\n",
      "Greater than 0 test passed for the table\n",
      "\n",
      "Running quality test for immi_dates_dm table\n",
      "immi_dates_dm has unique rows, and primary key constraint is not violated\n",
      "date_date doesnt have any nulls\n",
      "datatype match for column date_date having TimestampType values\n",
      "Greater than 0 test passed for the table\n",
      "\n",
      "Running quality test for ports_dm table\n",
      "ports_dm has unique rows, and primary key constraint is not violated\n",
      "code doesnt have any nulls\n",
      "Greater than 0 test passed for the table\n",
      "\n",
      "Running quality test for visas_dm table\n",
      "visas_dm has unique rows, and primary key constraint is not violated\n",
      "code doesnt have any nulls\n",
      "Greater than 0 test passed for the table\n",
      "\n",
      "Running quality test for airlines_dm table\n",
      "airlines_dm has unique rows, and primary key constraint is not violated\n",
      "flight_id doesnt have any nulls\n",
      "Greater than 0 test passed for the table\n",
      "\n",
      "Running quality test for demographics_dm table\n",
      "demographics_dm has unique rows, and primary key constraint is not violated\n",
      "state_code doesnt have any nulls\n",
      "Greater than 0 test passed for the table\n"
     ]
    }
   ],
   "source": [
    "run_quality_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is available in dataDictionary.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* \n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Rationale for the choice of tools and technologies for the project:\n",
    "    - Used apache spark for the job on EMR cluster, as EMR is easy to setup and takes care of spark dependencies.\n",
    "    - Spark use was made because of its ability to process big chunks of data at speed and in memory, and its ability to use ditributed computing. \n",
    "    - Spark can also assist in ML, so keeping the analytical use cases for future and its adoptability, its easy to say at this point spark is not going away in the near future, due to its robust framework and operability with other tools. Its a perfect big data technology.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "     - The I94 immigration data is updated on a monthly basis and hence it is feasible to say, data processing and ETL can be done on a monthly basis.\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * Given the data was increased by 100x.\n",
    "    - Spark can handle the increase but we would consider increasing the number of nodes and memory of worker nodes in our cluster.\n",
    "    - If a tipping point comes, and since data is required for analytical purposes, and monthly, can change the pipeline design and do pre processing and transformation on a daily basis, and then do a batch processing every month.\n",
    "    - Further can explore use of cassandra, but is a beast of its own and need to assess effort vs reward\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - Can utilise a pipeline orchestrator and scheduling framwork such as airflow or luigi.\n",
    "    - Can setup SLAs in airflow if data is not updated in an hour for example pipeline will fail with error\n",
    "    - If two days data is delayed for example, airlow and luigi both should be able to handle backfills.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "    - We can use redshift for the above scenario since it is a scalable database. We can scale more if more requirement is there\n",
    "    - If demand further increases we know a no sql database on cloud like cassandra or a managed service should be a way to go, but costing and engineering effort needs to be taken into account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
